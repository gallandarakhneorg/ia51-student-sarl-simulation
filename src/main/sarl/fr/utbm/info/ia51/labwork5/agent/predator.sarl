/* 
 * $Id$
 * 
 * Copyright (c) 2014-17 Stephane GALLAND <stephane.galland@utbm.fr>.
 * 
 * This library is free software; you can redistribute it and/or
 * modify it under the terms of the GNU Lesser General Public
 * License as published by the Free Software Foundation; either
 * version 2.1 of the License, or (at your option) any later version.
 * 
 * This library is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * Lesser General Public License for more details.
 * 
 * You should have received a copy of the GNU Lesser General Public
 * License along with this library; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 * This program is free software; you can redistribute it and/or modify
 */
package fr.utbm.info.ia51.labwork5.^agent

import fr.utbm.info.ia51.framework.^agent.AbstractAnimat
import fr.utbm.info.ia51.framework.^agent.MotionAlgorithmOutput
import fr.utbm.info.ia51.framework.environment.DynamicType
import fr.utbm.info.ia51.framework.environment.PerceptionEvent
import fr.utbm.info.ia51.framework.math.MathUtil
import fr.utbm.info.ia51.framework.math.Point2f
import fr.utbm.info.ia51.general.motionalgo.KinematicSeekAlgorithm
import fr.utbm.info.ia51.general.motionalgo.KinematicWanderAlgorithm
import fr.utbm.info.ia51.general.motionalgo.SeekAlgorithm
import fr.utbm.info.ia51.general.motionalgo.SteeringSeekAlgorithm
import fr.utbm.info.ia51.general.motionalgo.SteeringWanderAlgorithm
import fr.utbm.info.ia51.general.motionalgo.WanderAlgorithm
import fr.utbm.info.ia51.general.qlearning.DefaultQState
import fr.utbm.info.ia51.general.qlearning.QLearning
import io.sarl.core.Initialize

agent Predator extends AbstractAnimat {
	
	val STOP_RADIUS = MathUtil.PI/10f
	val SLOW_RADIUS = MathUtil.PI/4f
	val WANDER_CIRCLE_DISTANCE = 20f
	val WANDER_CIRCLE_RADIUS = 10f
	val WANDER_MAX_ROTATION = MathUtil.PI/4f
	
	val NUMBER_OF_QLEARNING_ITERATIONS = 5
	val DISTANCE_WHEN_GOING_AROUND = 100f;

	var seekBehaviour : SeekAlgorithm
	var wanderBehaviour : WanderAlgorithm
		
	/** The instance of the learning problem to use.
	 */
	var qProblem : PredatingProblem
	
	/** The learning algorithm.
	 */
	var qLearning : QLearning<DefaultQState,PredatorAction>
	
	/** Last encountered target.
	 */
	var lastTarget : Point2f = null
	
	override overridableInitializationStage(it : Initialize) {
		if (this.behaviorType == DynamicType::STEERING) {
			this.seekBehaviour = new SteeringSeekAlgorithm
			this.wanderBehaviour = new SteeringWanderAlgorithm(
					WANDER_CIRCLE_DISTANCE, 
					WANDER_CIRCLE_RADIUS, 
					WANDER_MAX_ROTATION,
					STOP_RADIUS,
					SLOW_RADIUS)
		}
		else {
			this.seekBehaviour = new KinematicSeekAlgorithm
			this.wanderBehaviour = new KinematicWanderAlgorithm
		} 
		
		this.qProblem = new PredatingProblem();
		this.qLearning = new QLearning<DefaultQState,PredatorAction>(this.qProblem);
	}
		
	on PerceptionEvent {
		val position = occurrence.body.position
		val orientation = occurrence.body.direction
		val linearSpeed = occurrence.body.currentLinearSpeed
		val angularSpeed = occurrence.body.currentAngularSpeed
		
		this.qProblem.translateCurrentState(position, occurrence.perceptions)
		
		this.qLearning.learn(NUMBER_OF_QLEARNING_ITERATIONS)
		
		var action = this.qLearning.getBestAction(this.qProblem.currentState)
		
		var target = this.qProblem.lastEncounteredTarget
		this.lastTarget = null;
		
		var output : MotionAlgorithmOutput = null
		
		if (action !== null) {
			switch (action.type) {
				
			case RANDOM_MOVE: {
				// run the seek behaviour
				output = this.wanderBehaviour.run(
					position, orientation, 
					linearSpeed, occurrence.body.maxLinear,
					angularSpeed, occurrence.body.maxAngular)
			}

			case MOVE_TO_KILL_THE_PREY: {
				// run the seek behaviour
				if (target === null) {
					this.qLearning.forceFeedBack(this.qProblem.currentState, action, -5)
				}
				else {
					//this.lastTarget = new Point2d(target.getPosition());
					//output = this.seekBehaviour.runSeek(position, linearSpeed, getMaxLinear(), this.lastTarget);
					var v = position.operator_minus(target.position).toColinearVector(DISTANCE_WHEN_GOING_AROUND)
					this.lastTarget = target.position.operator_plus(v)
					output = this.seekBehaviour.run(
						position,
						linearSpeed, occurrence.body.maxLinear,
						this.lastTarget)
				}
			}

			case MOVE_LEFT: {
				if (target === null) {
					this.qLearning.forceFeedBack(this.qProblem.currentState, action, -5);
				}
				else {
					var v = position.operator_minus(target.position).toColinearVector(-DISTANCE_WHEN_GOING_AROUND)
					v.perpendicularize
					this.lastTarget = target.position.operator_plus(v)
					output = this.seekBehaviour.run(
						position,
						linearSpeed, occurrence.body.maxLinear,
						this.lastTarget);
				}
			}

			case MOVE_RIGHT: {
				if (target === null) {
					this.qLearning.forceFeedBack(this.qProblem.currentState, action, -5)
				}
				else {
					var v = position.operator_minus(target.position).toColinearVector(DISTANCE_WHEN_GOING_AROUND)
					v.perpendicularize
					this.lastTarget = target.position.operator_plus(v)
					output = this.seekBehaviour.run(
						position,
						linearSpeed, occurrence.body.maxLinear,
						this.lastTarget);
				}
			}
			default: {
				// Do nothing.
				if (target !== null) {
					var v = position.operator_minus(target.position).toColinearVector(DISTANCE_WHEN_GOING_AROUND)
					this.lastTarget = target.position.operator_plus(v)
					output = this.seekBehaviour.run(
						position,
						linearSpeed, occurrence.body.maxLinear,
						this.lastTarget);
				}
			}
			}
		}

		// Send the behaviour output to the environment
		output.emitInfluence
	}

}